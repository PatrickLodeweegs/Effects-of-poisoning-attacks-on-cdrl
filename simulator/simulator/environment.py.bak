#!/usr/bin/env python3
# from argparse import ArgumentParser
# import time
import random
from typing import Tuple,List, Dict, Union

import numpy as np
import pandas as pd
import torch
# from torch.utils.data import DataLoader, random_split
import gymnasium as gym
from gymnasium.utils import seeding

from simulator.model import LMF
from simulator.dataloaders import MovieLensDataset
import simulator.utils


class SimulatorEnv2(gym.Env):
    def __init__(
        self,
        model_path: str,
        dataset: str,
        seed: int = 1234,
        rho: float = 0.5,
        max_depth: int = 10,
        n_factors: int = 30
    ):
        self.seed(seed)
        
        self.dataset = MovieLensDataset(dataset)
        self.model = LMF(num_users=self.dataset.num_users, 
                         num_items=self.dataset.num_items,
                         num_factors=n_factors)
        self.model.load_state_dict(torch.load(model_path))
        
        self.delta: float = round(random.uniform(0, 1), 3)
        self.rho: float = rho
        # self.simLoader = None
        # self.depth = 0
        self.max_depth: int = max_depth

        self.reset()
        obs_low = np.concatenate(
            ([0] * self.model.P.embedding_dim, [0] * self.model.Q.embedding_dim)
        )
        obs_high = np.concatenate(
            ([1] * self.model.P.embedding_dim, [1] * self.model.Q.embedding_dim)
        )
        self.observation_space = gym.spaces.Box(
            low=obs_low, high=obs_high, dtype=np.float32
        )
        # self.action_space = gym.spaces.Box(low = -1, high = 1,
        # shape = (self.model.Q.embedding_dim), dtype = np.float32)
        self.action_space = gym.spaces.Box(-1., 1., shape=(self.model.Q.num_embeddings,), dtype='float32')
        # self.action_space = gym.spaces.Discrete(self.model.Q.num_embeddings)
        # self.prev_interactions = [self._get_item_vec(self.initial_item)]
    

    def seed(self, s=None):
        random.seed(s)
        # np.seed(s)
        # torch.seed(s)
        return [s]
    
    def _get_obs(self):
        """Create a new state based on the user vector and the most recent previous interaction"""
        print(self.user_vec.shape, self.prev_interactions[-1].shape)
        return np.concatenate((self.user_vec, self.prev_interactions[-1]))

    def _get_item_vec(self, item):
        """Predict the item vector based on the user id and item id"""
        return self.model.Q(torch.tensor([item])).detach().numpy()

    def reset(self, seed=None, options=None):
        """Reset the environment.
        We clear the prev interactions, set a new user, delta and initial item
        """
        self.seed(seed)
        self.delta = round(random.uniform(0, 1), 3)
        self.interaction_length = round(random.normalvariate(9, 4))
        # Get a random id in the valid range
        self.user = random.randint(0, self.model.P.num_embeddings)
        # with torch.no_grad():
        self.user_vec = self.model.P(torch.tensor([self.user])).detach().numpy()
            # raise ValueError(type(self.user_vec))
        self.depth = 0
        # random.shuffle(self.action_space)
        self.initial_item = random.randint(0, self.model.Q.num_embeddings)
        self.prev_interactions = [self._get_item_vec(self.initial_item)]

        observation = self._get_obs()
        return observation, None

    def step(self, action):
        """Single interaction with the model."""
        action = np.clip(action, self.action_space.low, self.action_space.high)
        vi = self._get_item_vec(action)
        # self.action_space.remove(action)
        self.depth += 1
        with torch.no_grad():
            pi = self.model(self.user, action)
            if self.prev_interactions:
                cur = self.delta * pi
                norm = (1 - self.delta) / len(self.prev_interactions)
                past = sum(vi @ vj.T for vj in self.prev_interactions)
                pi = cur + norm * past

        if pi > self.rho:
            reward = 1
        else:
            reward = 0
        self.prev_interactions.append(vi)
        observation = self._get_obs()
        return observation, reward, self.depth >= self.max_depth, False, None

    def render(self):
        raise NotImplementedError()



class SimulatorEnv(gym.Env):
      # Environment static properties
    metadata = {'render.modes': ['human', 'logger']}
    id = 'reco-v0'
    actions = np.eye(5)

    def __init__(self,
                model_path: str,
                dataset: str,
                seed: int = 1234,
                rho: float = 0.5,
                max_depth: int = 10,
                n_factors: int = 30,):
        """
        Parameterized constructor
        """
        # data for creating features
        self.dataset = MovieLensDataset(dataset)
        self.data = self.dataset.data
        self.item = self.dataset.item
        self.user = self.dataset.user
        # features derived from data
        self.movie_genre = self._get_movie_genre(item=self.item)
        self.user_info = self._get_user_data(user=self.user)
        self.occupations = self.user.occupation.unique().tolist()
        self.num_of_occupations = len(self.occupations)
        self.user_mean = self.data.groupby('user_id').mean().to_dict()['rating']
        self.movie_mean = self.data.groupby('item_id').mean().to_dict()['rating']
        # MDP variables
        self.reward = 0.0
        self.done = False
        self.observation = None
        self.action = 0
        # other environment variables
        self.local_step_number = 0
        self._seed = seed
        self._random_state = np.random.RandomState(seed=self._seed)
        self.max_step = self.data.shape[0] - 2
        self.total_correct_predictions = 0
        # convert data to numpy for faster training
        self.data = self.data.values
        # other openAI.gym specific variables
        self.action_space = gym.spaces.Box(0., 1., shape=(len(SimulatorEnv.actions),), dtype='float32')
        # self.action_space = gym.spaces.Discrete(len(SimulatorEnv.actions))
        self.observation_space = gym.spaces.Box(low=-1., high=5.0,
                                            shape=self._get_observation(
                                                step_number=0).shape,
                                            dtype=np.float32)

    def step(self, action: int = 0) -> Tuple[np.ndarray, float, bool, dict]:
        """
        Agent steps through environment
        """
        if self.done:
            self.observation = self.reset()
            return self.observation, self.reward, self.done, {}
        # self.action = action
        # print("ACTION: ", action)
        action = np.random.choice(len(action), p=action / action.sum())

        self.reward = self._get_reward(action=action, step_number=self.local_step_number)
        self.observation = self._get_observation(step_number=self.local_step_number)
        if self.reward > 0.:
            self.total_correct_predictions += 1
        if self.local_step_number >= self.max_step:
            self.done = True
        self.local_step_number += 1
        return self.observation, self.reward, self.done, {}

    def reset(self) -> np.ndarray:
        """
        Reset the environment to an initial state
        """
        self.local_step_number = 0
        self.reward = 0.0
        self.done = False
        print(f"Reco is being reset() --> "
              f"first step = {self.local_step_number} | "
              f"Total_correct = {self.total_correct_predictions}")
        self.total_correct_predictions = 0
        return self._get_observation(step_number=self.local_step_number)

    def render(self, mode: str = 'human') -> None:
        """
        Render environment
        """
        if mode == 'logger':
            print(f"Env observation at step {self.local_step_number} is \n{self.observation}")

    def close(self) -> None:
        """
        Clear resources when shutting down environment
        """
        self.data = None
        self.user = None
        self.item = None
        print("RecoGym is being closed.")

    def seed(self, seed: int = 1) -> List[int]:
        """
        Set random seed
        """
        self._random_state = np.random.RandomState(seed=seed)
        self._seed = seed
        return [seed]

    def __str__(self) -> str:
        return f'GymID={RecoEnv.id} | seed={self._seed}'

    @staticmethod
    def _one_hot(num: int, selection: int) -> np.ndarray:
        """
        Create one-hot features
        """
        return np.eye(num, dtype=np.float32)[selection]

    @staticmethod
    def _get_movie_genre(item: pd.DataFrame) -> Dict[int, np.ndarray]:
        """
        Extract one-hot of movie genre type from dataset
        """
        movie_genre = dict([(movie_id, np.empty(19, dtype=np.float32))
                            for movie_id in item['movie_id'].tolist()])
        for movie_id in range(1, len(movie_genre)):
            movie_genre[movie_id] = item.iloc[movie_id, 5:].values.astype(np.float32)
        return movie_genre

    @staticmethod
    def _get_user_data(user: pd.DataFrame) -> Dict[int, Dict[str, Union[int, str]]]:
        """
        Create dictionary of user stats (e.g., age, occupation, gender)
        to use as inputs into other functions.
        """
        tmp_user = user.drop(['zip_code'], axis=1)
        tmp_user.index = tmp_user.user_id
        tmp_user = tmp_user.drop(['user_id'], axis=1)
        return tmp_user.to_dict(orient='index')

    def _get_movie_genre_buckets(self, movie_id: int = 1) -> np.ndarray:
        """
        Extract one-hot of movie genre type for a specific movie_id
        """
        return self.movie_genre.get(movie_id, np.empty(19, dtype=np.float32))

    def _get_age_buckets(self, age: int = 10) -> np.ndarray:
        """
        Extract one-hot of age group for a specific age
        """
        if age < 10:
            bucket_number = 0
        elif age < 20:
            bucket_number = 1
        elif age < 30:
            bucket_number = 2
        elif age < 40:
            bucket_number = 3
        elif age < 50:
            bucket_number = 4
        elif age < 60:
            bucket_number = 5
        else:
            bucket_number = 6
        return self._one_hot(num=7, selection=bucket_number)

    def _get_occupation_buckets(self, job: str = 'none') -> np.ndarray:
        """
        Extract one-hot of occupation type for a specific job
        """
        selection = self.occupations.index(job)
        return self._one_hot(num=self.num_of_occupations, selection=selection)

    def _get_gender_buckets(self, gender: str = 'm') -> np.ndarray:
        """
        Extract one-hot of gender type for a specific gender (e.g., M or F)
        """
        sex = gender.upper()
        sex_id = 0 if sex == 'M' else 1
        return self._one_hot(num=2, selection=sex_id)

    def _get_observation(self, step_number: int = 0) -> np.ndarray:
        """
        Get features and concatenate them into one observation

        Features=
          user_mean:
            Average rating given by a specific user_id
          movie_mean:
            Average rating for a specific movie_id
          movie_genre_bucket:
            One-hot of the movie type
          age_bucket:
            One-hot of user's age range
          occupation_bucket:
            One-hot of the user's job
          gender_bucket:
            One-hot of the user's gender (only M or F)
        """
        # lookup keys
        user_id = self.data[step_number, 0]
        movie_id = self.data[step_number, 1]
        # values for one_hot
        user_age = self.user_info[user_id]['age']
        user_occupation = self.user_info[user_id]['occupation']
        user_gender = self.user_info[user_id]['gender']
        # features
        user_mean = np.array([self.user_mean.get(user_id, 3.) / 5.], dtype=np.float32)
        movie_mean = np.array([self.movie_mean.get(movie_id, 3.) / 5.], dtype=np.float32)
        movie_genre_bucket = self._get_movie_genre_buckets(movie_id=movie_id)
        age_bucket = self._get_age_buckets(age=user_age)
        occupation_bucket = self._get_occupation_buckets(job=user_occupation)
        gender_bucket = self._get_gender_buckets(gender=user_gender)
        # concatenate it all together
        return np.concatenate((user_mean, movie_mean, movie_genre_bucket,
                               age_bucket, occupation_bucket, gender_bucket))

    def _get_reward(self, action: int, step_number: int) -> float:
        """
        Calculate reward for a given state and action
        """
        users_rating = int(self.data[step_number, 2])
        # print("\n", action, "\n")
        predicted_rating = int(action) + 1  # to compensate for zero-index
        prediction_difference = abs(predicted_rating - users_rating)
        reward = 0.
        if prediction_difference == 0:
            reward += 1.
        elif prediction_difference != 5:
            # Use addition since log loss is negative
            reward += np.log(1. - prediction_difference / 5)
        return reward